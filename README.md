<!-- HEADER -->
<div align="center">
  <img src="https://media.glamour.com/photos/5b9a4ffed4057a0ec3d2deee/16:9/w_800,h_450,c_limit/fenty-river.gif" height=400>
  
  #
  <h1>
    ğŸ«¶ğŸ»ğŸ«¶ğŸ¼ğŸ«¶ğŸ«¶ğŸ½ğŸ«¶ğŸ¾ğŸ«¶ğŸ¿<br>Equitable AI for Dermatology<br>ğŸ«¶ğŸ¿ğŸ«¶ğŸ¾ğŸ«¶ğŸ½ğŸ«¶ğŸ«¶ğŸ¼ğŸ«¶ğŸ»
    <h3>Break Through Tech AI Algorithmic Justice League Team 12</h3>
  </h1>
  <br><br><br><br>
</div>

#
<!-- CONTRIBUTORS -->
<div align="center">
  <h1>
    ğŸ¤ğŸ¿ Contributors ğŸ¤ğŸ¿
  </h1>
</div>


### [Fridah Ntika](https://www.github.com/FridahNtika)
Fridah (she/her) is a Data Science student at Wellesley College. A fun fact about her is that she loves to travel and has visited over 10 countries!
<br><br>
### [Ishita Kakkar](https://github.com/ikakkar03)
Ishita (she/her) is an honors Computer Science and Math student at UMass Amherst. A fun fact about her is that she is one of the captains of our team!
<br><br>
### [Ajibola Falade](https://www.github.com/Blash03)
Ajibola (he/him) is a Computer Science student at Brandeis University. A fun fact about him is that he enjoys jazz music and has played the saxophone ever since he started school!
<br><br>
### [Kerem Farzaliyev](https://www.github.com/KeremFarzaliyev)
Kerem (he/him) is an Engineering Physics and Economics student at UMass Boston. A fun fact about him is that he loves learning languages!
<br><br>
### [Paula Sefia](https://www.github.com/psefia)
Paula (she/her) is a Computer Science and Cognitive Psychology student at Northeastern University. A fun fact about her is that she used to play 5 instruments in grade and middle school!
<br><br><br>

[ğŸ”¼ Back to top](#top)

<br>


<!-- BREAK THROUGH TECH AI-->
<div align="center">
  
  #
  <h1>
    ğŸ™ŒğŸ» What is Break Through Tech AI? ğŸ™ŒğŸ»
  </h1>
</div>

[<img align="left" src="https://github.com/user-attachments/assets/e239a754-d90c-4c23-94ca-820ee7d5d052" height=210>](https://www.breakthroughtech.org/programs/the-ai-program/)

Break Through Tech AI is a Cornell Tech initiative that brings together college students in the greater Boston area interested in taking the first steps to enter the Artificial Intelligence (AI) and Machine Learning (ML) industry. It is a virtual, one-year extracurricular experience that equips students with the skills needed to secure a job in the fastest-growing areas of tech. Developed by industry and academic leaders, students master the skills necessary to become a successful machine learning engineer. This program is split into 3 sections: An ML Foundations course sponsored by Cornell, an industry project in the fall, and a Google Kaggle competition in the spring (hence, this very project). 
<br><br>
<div align="center">
  <h3>
    Learn more about Break Through Tech and its subsidiaries by clicking on the logo above!
  </h3>
</div>

<br><br><br>

[ğŸ”¼ Back to top](#top)

<br>

<!-- ALGORITHMIC JUSTICE LEAGUE -->
<div align="center">
  
  #
  <h1>
    âœŠğŸ¾ What is the Algorithmic Justice League? âœŠğŸ¾
  </h1>
</div>

[<img align="left" src="https://media.licdn.com/dms/image/v2/C5112AQFcWX0i6N4Dcg/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1520243519212?e=2147483647&v=beta&t=9bXNPCIz_VNyhstF7qXp2A_4wUwBv2Q0O02Y2YSs7wY" height=210>](https://www.ajl.org/about)
<br><br>
Founded by Joy Buolamwini, the Algorithmic Justice Leagueâ€™s mission is to raise public awareness about the impacts of AI, equip advocates with resources to bolster campaigns, build the voice and choice of the most impacted communities, and galvanize researchers, policymakers, and industry practitioners to prevent AI harms.
<br><br><br><br><br>
<div align="center">
  <h3>
    Learn more about the Algorithmic Justice League by clicking on the picture above!
  </h3>
</div>

<br><br>

[ğŸ”¼ Back to top](#top)

<br>

<!-- RESOURCES -->
<div align="center">
  
  #
  <h1>
    ğŸ¤³ğŸ» Resources ğŸ¤³ğŸ»
  </h1>
 
  [<img src="https://static1.howtogeekimages.com/wordpress/wp-content/uploads/2023/11/17-1.png" height=150>](https://drive.google.com/drive/u/2/folders/0AF4tjg4-0DrdUk9PVA)
  [<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRFoP3fDlWVMF71lXyxD7Qp-S-FnFbarYTfww&s" height=150>](https://www.notion.so/AJL-Team-12-Project-1866596d1086800280defde41889322e)
  [<img src="https://cdn.prod.website-files.com/5fc212183117036dc3c635d0/611f5ebe2709f0744c0f4413_Sourcing%20talent%20on%20Kaggle%20image.png" height=150>](https://www.kaggle.com/competitions/bttai-ajl-2025/overview)

</div>

<br><br>

[ğŸ”¼ Back to top](#top)

<br>

<!-- SETUP -->
Install all necessary dependencies, including Python 3.7+ and the required libraries such as PyTorch and timm. Place the training images under bttai-ajl-2025/train/train/<disease_class> and the test images under bttai-ajl-2025/test/test, and ensure that train.csv and test.csv (containing at least "md5hash" and "fitzpatrick_scale") are in the bttai-ajl-2025 folder. Run the training script or notebook cell that reads train.csv, scans each disease class subfolder, applies mild augmentations, trains the chosen model (for example, a Swin Transformer), and saves the best checkpoint (such as best_model_swin.pt). Load this saved checkpoint in an inference script or notebook cell, read each test image using test.csv and the test folder, pass them through the model to obtain predictions, and write a submission.csv file with two columns: "md5hash" and "label". Finally, submit or evaluate the resulting submission.csv against the test set.
[ğŸ”¼ Back to top](#top)


<!-- EQUITABLE AI FOR DERMATOLOGY -->
<div align="center">
  
  #
  <h1>
    ğŸ–ï¸ Equitable AI for Dermatology ğŸ–ï¸
  </h1>
</div>

<img align="left" src="https://www.enrichclinic.com.au/wp-content/uploads/2023/08/iStock-1410979275.jpg" height=210>

AI is transforming healthcare, yet dermatology AI tools often underperform for people with darker skin tones due to a lack of diverse training data. This can lead to diagnostic errors, delayed treatments, and health disparities for underserved communities. To combat the diagnostic errors, the goal is to train a model that can, as accurately as possible, classify 21 different skin conditions across diverse skin tones, as denoted by the Fitzpatrick Scale. The Fitzpatrick Scale, developed in 1975 by American dermatologist Thomas B. Fitzpatrick, is an objective classification system to denote how much melanin is present in one's skin, the effects of sun exposure on their skin, and the likelihood for them to get tanned or sunburnt by constant sun exposure. The image on the left shows the 6 categories of skin colors as denoted by the Fitzpatrick Scale. 

<br><br><br>
[ğŸ”¼ Back to top](#top)

<br>

<!-- HISTORY OF MISCLASSIFICATION -->
<div align="center">
  
  #
  <h1>
    âœŠğŸ¿ Consequences of Misclassification âœŠğŸ¿
  </h1>
</div>

<img align="left" src="https://miro.medium.com/v2/resize:fit:1400/0*_4G2kAbxNdu8gB0A" height=250>

The purpose of Break Through Tech AI is to incorporate diverse perspectives that are often overlooked into the AI industry. It is quite fitting, as a result, that this project is related to mitigating the issue of misclassified skin conditions. Skin diseases on darker skin tend to be overlooked due to factors like less representation in datasets that models are trained on, the conditions being camouflaged by the darker skin, or people's assumptions of people with darker skin having less conditions due to their resistance to UV exposure. However, these skin conditions are still real and very much present, and training models that are unable to quantify that fairly could cause people with darker skin to suffer in silence with conditions they either do not understand or did not know were even present. 

<br><br><br>
[ğŸ”¼ Back to top](#top)

<br>

<!-- THE DATASET -->
<div align="center">
  
  #
  <h1>
    âœğŸ» The Dataset âœğŸ»
  </h1>
</div>

<img align="left" src="https://cdn.discordapp.com/attachments/1333521648980328538/1335004586150199358/Fitzpatrick_Centaur.png?ex=67dddf68&is=67dc8de8&hm=f59a0829aa914055ed0798c0fd357d76ef40f82a02023faf48a4ede2c487f21a&format=webp&quality=lossless&width=880&height=684" height=400>

Prior image classification models typically use datasets, such as the one provided to us, to train and test how well the model can classify new skin conditions based on the knowledge it has learned from previous training. The dataset we were given is a subset of the FitzPatrick17k dataset, which contains approximately 17,000 images of various dermatological conditions, ranging from serious to cosmetic, across a spectrum of skin tones according to the Fitzpatrick scale. For this competition, we worked with a subset of 4,500 images from that dataset. These images are categorized by both their corresponding Fitzpatrick skin tone scale and one of the 21 skin conditions included in the subset. The challenge creators intentionally curated the subset by adding more images of individuals with lighter skin tones (i.e., lower Fitzpatrick scale scores) than darker tones. This decision was made to address a well-documented gap in machine learning research, where there is often less data available on dermatological conditions for people with darker skin tones. As illustrated in the graph on the left, which shows the distribution of images across the Fitzpatrick skin tone categories, this imbalance in data is visible. Consequently, our team chose to address this issue by balancing the dataset using various tools we researched, ensuring an equitable representation of images across all Fitzpatrick skin tones.

<br><br><br>
[ğŸ”¼ Back to top](#top)
<br>

<!-- DATA MANIPULATION STRATEGIES -->
We first identified that images representing lower Fitzpatrick scale classes were underrepresented in our dataset, leading to potential biases in model training. To address this issue, we applied a WeightedRandomSampler, which ensured that images from the underrepresented darker skin tones were sampled more frequently during training. Additionally, we tested various augmentations, including horizontal flips and slight color jitter. However, we discovered that excessive color or rotation distortions sometimes harmed the modelâ€™s performance, likely because subtle color and orientation cues are essential in dermatology. As a result, we ultimately used only mild augmentations to balance the need for variability with the preservation of medically relevant features.
[ğŸ”¼ Back to top](#top)

<!-- MODEL -->
We initially experimented with basic CNN architectures (such as ResNet and DenseNet) and MLPâ€style models (like ResMLP and Mixer), which provided early baselines but did not reach high accuracy on our relatively small dataset. We then tried Vision Transformers (ViT and Swin) and found that the Swin Transformer consistently achieved better results in singleâ€split experiments. We also explored ConvNeXt, but in our tests, it did not outperform the Swin Transformer. Ultimately, our bestâ€performing approach used the Swin Transformer backbone, a small embedding layer for the Fitzpatrick scale input, and mild data augmentations. This setup achieved a test accuracy of around 76% when trained with a learning rate of approximately 3eâ€5, label smoothing of 0.1, and no partial freezing of the backbone.
[ğŸ”¼ Back to top](#top)

<!-- VISUALIZATIONS -->
In order to understand the modelâ€™s strengths and weaknesses, we produced confusion matrices that illustrated improved coverage of darker Fitzpatrick classes after applying the WeightedRandomSampler. We also plotted training loss and accuracy curves, confirming that the model converged around epochs 15â€“20. For completeness, we generated sample augmented images to ensure that the horizontal flips and minimal color jitter did not excessively distort critical medical features.
[ğŸ”¼ Back to top](#top)

<!-- ACCURACY SCORE -->
<div align="center">
  
  #
  <h1>
    The Accuracy Score
  </h1>
</div>

<img align="left" src="ajl leaderboard.png" height=250>

To evaluate our model's performance, we used the F1 score, harmonic mean of precision and recall, providing a single metric that balances false positives and false negatives. A score of 0.75953 (~76%) suggests the model is making correct predictions about 76% of the time in terms of how well it balances precision and recall. Our model shows promise in classifying skin conditions across diverse skin tones, but there is still room for improvement.

<br><br><br>
[ğŸ”¼ Back to top](#top)
<br>

<!-- CHALLENGES/LIMITATIONS -->
One of our main challenges was integrating fairness metrics and tools (like fairlearn) to measure and mitigate performance disparities across Fitzpatrick scales. Although we initially planned to use these frameworks, we found that our small dataset and the complexity of advanced fairness mitigation techniques made it difficult to incorporate them without sacrificing overall accuracy. We also recognized that single train/val splits can produce deceptively high validation accuracy if the split is â€œlucky.â€ In contrast, crossâ€validation often yields lower but more reliable estimates of performance on each fold, which may ultimately lead to better generalization if models are ensembled. Finally, we acknowledged that the domain variability in dermatology imagesâ€”particularly the subtle color and orientation cuesâ€”made it challenging to find the right balance of augmentations without harming the modelâ€™s ability to learn medically relevant patterns.
[ğŸ”¼ Back to top](#top)

<!-- FUTURE AJL DIRECTIVES -->
<div align="center">
  
  #
  <h1>
    What Can AJL Do and How Can It Advance Equity By Centering Those Historically Excluded In AI?
  </h1>
</div>

Despite AIâ€™s transformative potential in healthcare, dermatology models have historically failed people with darker skin due to biased datasets and a lack of representation. Our work directly addresses this gap and aligns with AJLâ€™s mission to prevent AI harms and build equitable AI systems that serve historically excluded communities.

<br><br><br>
[ğŸ”¼ Back to top](#top)
<br>

<!-- REFERENCES -->
Mansouri, O., AsgarianDehkordi, H., Kolaei, T., Xiao, Y., & Rivaz, H. (2023). Medical Image Classification with KAN-Integrated Transformers and Dilated Neighborhood Attention. arXiv:2304.2025

Chen, S., Wang, H., et al. (2021). TransMed: Transforming Multi-Modal Medical Image Classification. arXiv:2104.03807

Ã‡iÃ§ek, Ã–., Abdulkadir, A., Lienkamp, S., Brox, T., & Ronneberger, O. (2016). 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. In Medical Image Computing and Computer-Assisted Intervention â€“ MICCAI 2016 (pp. 424â€“432).

Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention â€“ MICCAI 2015 (pp. 234â€“241).

Dosovitskiy, A., et al. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations (ICLR).

Liu, Z., Lin, Y., Cao, Y., et al. (2021). Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).

Liu, Z., Mao, H., Wu, C., et al. (2022). A ConvNet for the 2020s (ConvNeXt). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).

[ğŸ”¼ Back to top](#top)
